{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.4078e-45])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.0065e-45, 0.0000e+00, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 1.8754e+28],\n",
      "        [3.2175e+21, 8.3742e-10, 1.2914e-11]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9448, 0.6265],\n",
      "        [0.8853, 0.4670]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2,2)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, dtype=torch.float16)\n",
    "print(x.dtype)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.5, 0.1])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5189, 0.3550],\n",
      "        [0.2730, 0.9572]])\n",
      "tensor([[0.4177, 0.0694],\n",
      "        [0.5481, 0.5643]])\n",
      "tensor([[0.9365, 0.4244],\n",
      "        [0.8211, 1.5215]])\n",
      "tensor([[0.9365, 0.4244],\n",
      "        [0.8211, 1.5215]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "z = x+y\n",
    "z = torch.add(x,y)\n",
    "print(z)\n",
    "\n",
    "y.add_(x) # in-place addition\n",
    "print(y)\n",
    "\n",
    "z = x-y\n",
    "z = torch.sub(x,y)\n",
    "\n",
    "z = x*y\n",
    "z = torch.mul(x,y)\n",
    "y.mul_(x)\n",
    "\n",
    "z = torch.div(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7489, 0.6066, 0.9058],\n",
      "        [0.8456, 0.0597, 0.4443],\n",
      "        [0.6439, 0.0173, 0.9627],\n",
      "        [0.0030, 0.8649, 0.6860],\n",
      "        [0.7764, 0.8451, 0.1274]])\n",
      "tensor([0.7489, 0.8456, 0.6439, 0.0030, 0.7764])\n",
      "tensor(0.0597)\n",
      "0.059664130210876465\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "print(x[:,0])\n",
    "print(x[1,1])\n",
    "print(x[1,1].item()) # if only one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2641, 0.2050, 0.8822, 0.9938],\n",
      "        [0.4625, 0.3041, 0.5042, 0.4876],\n",
      "        [0.5462, 0.0337, 0.5629, 0.6748],\n",
      "        [0.1923, 0.3689, 0.2035, 0.4325]])\n",
      "torch.Size([16])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "\n",
    "y = x.view(16)\n",
    "print(y.size())\n",
    "\n",
    "y = x.view(-1, 8)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "\n",
    "b = a.numpy()\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "print(a)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, requires_grad=True)\n",
    "print(x)     # If gradient has to be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3367,  0.1288,  0.2345],\n",
      "        [ 0.2303, -1.1229, -0.1863],\n",
      "        [ 2.2082, -0.6380,  0.4617]]) \n",
      "\n",
      "tensor([[ 0.2674,  0.5349,  0.8094],\n",
      "        [ 1.1103, -1.6898, -0.9890],\n",
      "        [ 0.9580,  1.3221,  0.8172]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.randn(3,3)\n",
    "b = torch.randn(3,3)\n",
    "print(a, '\\n')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3367,  0.2303,  2.2082],\n",
       "        [ 0.1288, -1.1229, -0.6380],\n",
       "        [ 0.2345, -0.1863,  0.4617]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(a) # transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3367,  0.1288,  0.2345,  0.2674,  0.5349,  0.8094],\n",
       "        [ 0.2303, -1.1229, -0.1863,  1.1103, -1.6898, -0.9890],\n",
       "        [ 2.2082, -0.6380,  0.4617,  0.9580,  1.3221,  0.8172]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((a,b), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3367],\n",
       "        [ 0.1288],\n",
       "        [ 0.2345],\n",
       "        [ 0.2303],\n",
       "        [-1.1229],\n",
       "        [-0.1863],\n",
       "        [ 2.2082],\n",
       "        [-0.6380],\n",
       "        [ 0.4617]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape(9,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1590, 1.2798, 1.2302], requires_grad=True)\n",
      "tensor([4.1590, 3.2798, 3.2302], grad_fn=<AddBackward0>)\n",
      "tensor([34.5949, 21.5141, 20.8687], grad_fn=<MulBackward0>)\n",
      "tensor([1.6636e+00, 1.3119e+01, 1.2921e-02])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x+2\n",
    "print(y)\n",
    "\n",
    "z = y*y*2\n",
    "#z = z.mean()\n",
    "print(z)\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "z.backward(v) #dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5204, -0.2818, -0.6965], requires_grad=True)\n",
      "tensor([1.4796, 1.7182, 1.3035])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "#x.requires_grad_(False)\n",
    "#y = x.detach()\n",
    "#print(x)\n",
    "#print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x+2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    \n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass\n",
    "\n",
    "y_hat = w*x\n",
    "loss = (y_hat-y)**2\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# backward pass\n",
    "\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred-y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "lr = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    l.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= lr*w.grad\n",
    "        \n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline\n",
    "\n",
    "1) Design model (input, output size, forward pass) <br>\n",
    "2) Construct loss and optimizer <br>\n",
    "3) Training loop\n",
    "  - forward pass : compute prediction \n",
    "  - backward pass : gradients\n",
    "  - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = -2.788\n",
      "epoch 1: w = -0.053, loss = 51.97446442\n",
      "epoch 11: w = 1.601, loss = 1.35438085\n",
      "epoch 21: w = 1.869, loss = 0.04415482\n",
      "epoch 31: w = 1.914, loss = 0.00972569\n",
      "epoch 41: w = 1.923, loss = 0.00833534\n",
      "epoch 51: w = 1.926, loss = 0.00782884\n",
      "epoch 61: w = 1.929, loss = 0.00737261\n",
      "epoch 71: w = 1.931, loss = 0.00694347\n",
      "epoch 81: w = 1.933, loss = 0.00653933\n",
      "epoch 91: w = 1.935, loss = 0.00615871\n",
      "Prediction after training: f(5) = 9.869\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "#model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "lr = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters() , lr = lr)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    y_pred = model(X)\n",
    "    \n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    l.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 4414.7188\n",
      "epoch: 20, loss = 3295.4456\n",
      "epoch: 30, loss = 2484.8518\n",
      "epoch: 40, loss = 1897.2214\n",
      "epoch: 50, loss = 1470.8311\n",
      "epoch: 60, loss = 1161.1741\n",
      "epoch: 70, loss = 936.1155\n",
      "epoch: 80, loss = 772.4242\n",
      "epoch: 90, loss = 653.2880\n",
      "epoch: 100, loss = 566.5262\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "        \n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 0.5897\n",
      "epoch: 20, loss = 0.4864\n",
      "epoch: 30, loss = 0.4212\n",
      "epoch: 40, loss = 0.3761\n",
      "epoch: 50, loss = 0.3427\n",
      "epoch: 60, loss = 0.3169\n",
      "epoch: 70, loss = 0.2961\n",
      "epoch: 80, loss = 0.2790\n",
      "epoch: 90, loss = 0.2646\n",
      "epoch: 100, loss = 0.2522\n",
      "accuracy = 0.8860\n"
     ]
    }
   ],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "        #   \n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test) \n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    \n",
    "    acc = y_predicted_cls.eq(y_test).sum()/float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('wine.csv', delimiter=\",\", dtype=np.float32(), skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:,1:])\n",
    "        self.y = torch.from_numpy(xy[:,[0]])\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3270e+01, 4.2800e+00, 2.2600e+00, 2.0000e+01, 1.2000e+02, 1.5900e+00,\n",
      "         6.9000e-01, 4.3000e-01, 1.3500e+00, 1.0200e+01, 5.9000e-01, 1.5600e+00,\n",
      "         8.3500e+02],\n",
      "        [1.4220e+01, 3.9900e+00, 2.5100e+00, 1.3200e+01, 1.2800e+02, 3.0000e+00,\n",
      "         3.0400e+00, 2.0000e-01, 2.0800e+00, 5.1000e+00, 8.9000e-01, 3.5300e+00,\n",
      "         7.6000e+02],\n",
      "        [1.2510e+01, 1.7300e+00, 1.9800e+00, 2.0500e+01, 8.5000e+01, 2.2000e+00,\n",
      "         1.9200e+00, 3.2000e-01, 1.4800e+00, 2.9400e+00, 1.0400e+00, 3.5700e+00,\n",
      "         6.7200e+02],\n",
      "        [1.1820e+01, 1.7200e+00, 1.8800e+00, 1.9500e+01, 8.6000e+01, 2.5000e+00,\n",
      "         1.6400e+00, 3.7000e-01, 1.4200e+00, 2.0600e+00, 9.4000e-01, 2.4400e+00,\n",
      "         4.1500e+02]]) tensor([[3.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.]])\n"
     ]
    }
   ],
   "source": [
    "dataset = WineDataset()\n",
    "\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "#print(features, labels)\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "data = dataiter.next()\n",
    "\n",
    "features, labels = data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n",
      "epoch 1/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 45/45, inputs torch.Size([2, 13])\n",
      "epoch 2/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 45/45, inputs torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        if (i+1)%5 == 0:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataset Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, transform=None):\n",
    "        xy = np.loadtxt('wine.csv', delimiter=\",\", dtype=np.float32(), skiprows=1)\n",
    "        self.x = xy[:,1:]\n",
    "        self.y = xy[:,[0]]\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index], self.y[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "    \n",
    "class MulTransform:\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, targets      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "dataset = WineDataset(transform=ToTensor())\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "\n",
    "print(type(features), type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])\n",
    "dataset = WineDataset(transform=composed) \n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "\n",
    "print(type(features), type(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SoftMax and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38404151797294617\n",
      "1.9127614498138428\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "Y = torch.tensor([0])\n",
    "\n",
    "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n",
    "\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(l1.item())\n",
    "print(l2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "\n",
    "print(predictions1)\n",
    "print(predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3018244206905365\n",
      "2.061704158782959\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss() # Don't use softmax, Y_test is not one encoded\n",
    "\n",
    "# 3 samples\n",
    "Y = torch.tensor([2, 0, 1])\n",
    "\n",
    "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1], [2.0, 1.0, 0.1], [0.1, 3.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[2.1, 1.0, 0.1], [0.1, 1.0, 2.1], [2.0, 1.0, 0.1]])\n",
    "\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(l1.item())\n",
    "print(l2.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tanh()"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ReLU()\n",
    "nn.Sigmoid()\n",
    "nn.Tanh()\n",
    "\n",
    "torch.relu()\n",
    "torch.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Forward Prop Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transf\\orms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # 28x28\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root ='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root ='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "\n",
    "print(samples.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/2, steps 100/600, loss = 0.3873\n",
      "epoch 1/2, steps 200/600, loss = 0.3958\n",
      "epoch 1/2, steps 300/600, loss = 0.2240\n",
      "epoch 1/2, steps 400/600, loss = 0.2677\n",
      "epoch 1/2, steps 500/600, loss = 0.2422\n",
      "epoch 1/2, steps 600/600, loss = 0.1917\n",
      "epoch 2/2, steps 100/600, loss = 0.2211\n",
      "epoch 2/2, steps 200/600, loss = 0.4158\n",
      "epoch 2/2, steps 300/600, loss = 0.2068\n",
      "epoch 2/2, steps 400/600, loss = 0.1235\n",
      "epoch 2/2, steps 500/600, loss = 0.1890\n",
      "epoch 2/2, steps 600/600, loss = 0.3091\n",
      "accuracy = 0.94\n"
     ]
    }
   ],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, steps {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "    \n",
    "        # value, index\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct = (predictions==labels).sum().item()\n",
    "    \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'accuracy = {acc}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4\n",
    "batch_size = 4\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-e3075a395db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root ='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root ='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "model = ConvNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1)%2000 == 0:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, steps {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        \n",
    "        outputs = model(images)\n",
    "    \n",
    "        # value, index\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predictions==labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            \n",
    "            if (label == pred):\n",
    "                n_class_correct[label] +=1\n",
    "            \n",
    "            n_class_samples[label] += 1\n",
    "    \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'accuracy = {acc} %')\n",
    "    \n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "mean = np.array([0.5, 0.5, 0.5])\n",
    "std = np.array([0.25, 0.25, 0.25])\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(class_names)\n",
    "\n",
    "\n",
    "def imshow(inp, title):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "#### Finetuning the convnet ####\n",
    "# Load a pretrained model and reset final fully connected layer.\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# Learning rate scheduling should be applied after optimizer’s update\n",
    "# e.g., you should write your code this way:\n",
    "# for epoch in range(100):\n",
    "#     train(...)\n",
    "#     validate(...)\n",
    "#     scheduler.step()\n",
    "\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=25)\n",
    "\n",
    "\n",
    "#### ConvNet as fixed feature extractor ####\n",
    "# Here, we need to freeze all the network except the final layer.\n",
    "# We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward()\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Train and Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(arg, PATH) # tensor, dict, models\n",
    "\n",
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\"epoch\": 90, \"model_state\": model.state_dict(), \"optim_state:\": optimizer.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, \"checkpoint.pth\")\n",
    "loaded_checkpoint = torch.load(\"checkpoint.pth\")\n",
    "epoch = loaded_checkpoint[\"epoch\"]\n",
    "\n",
    "model = Model(n_input_features=6)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optim_state\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
